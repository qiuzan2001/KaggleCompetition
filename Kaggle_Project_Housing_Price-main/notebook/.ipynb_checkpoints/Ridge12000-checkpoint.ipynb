{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b074662e-6deb-4491-9460-e253b5b687e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# References:\n",
    "# \n",
    "# COMPREHENSIVE DATA EXPLORATION WITH PYTHON, Pedro Marcelino - February 2017\n",
    "# Link : https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n",
    "#\n",
    "# How I made top 0.3% on a Kaggle competition, Lavanya Shukla\n",
    "# Link : https://www.kaggle.com/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition/notebook\n",
    "#\n",
    "# Data Science Workflow TOP 2% (with Tuning), aqx\n",
    "# Link : https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning/notebook#4.-Modeling\n",
    "#\n",
    "# Machine Learning Explainability by Dan Becker, Kaggle Mini Course\n",
    "# ------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "# Misc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da18fc03-f31e-4e04-a6c6-1086a3270518",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Reading data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_train\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../input/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_test\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Quick correlation analysis\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/islp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/islp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/islp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/islp/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/islp/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './../input/train.csv'"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Reading data\n",
    "# ------------------------\n",
    "df_train=pd.read_csv('../input/train.csv')\n",
    "df_test=pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "# ------------------------\n",
    "# Quick correlation analysis\n",
    "# ------------------------\n",
    "corrmat=df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(10,12))\n",
    "sns.heatmap(corrmat,mask=corrmat<0.75,linewidth=0.5,cmap=\"Blues\", square=True)\n",
    "plt.show()\n",
    "\n",
    "corrmat=df_train.corr()\n",
    "print(corrmat['SalePrice'].sort_values(ascending=False).head(10))\n",
    "\n",
    "k = 10\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "f, ax = plt.subplots(figsize=(8,10))\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "                 annot_kws={'size': 12}, yticklabels=cols.values,\n",
    "                 xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395054f-63d9-4dae-b636-ccac40ae487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Outlier removal\n",
    "# ------------------------\n",
    "df_train.plot.scatter(x='GrLivArea', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "df_train=df_train.drop(df_train[df_train['Id']==1299].index)\n",
    "df_train=df_train.drop(df_train[df_train['Id']==524].index)\n",
    "\n",
    "df_train.plot.scatter(x='TotalBsmtSF', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "df_train.sort_values(by='TotalBsmtSF',ascending=False)[:2]\n",
    "df_train[df_train['TotalBsmtSF']>2000].sort_values(by='SalePrice',ascending=True)[:2]\n",
    "df_train.drop(df_train[df_train['Id']==1224].index,inplace=True)\n",
    "df_train.plot.scatter(x='TotalBsmtSF', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=df_train)\n",
    "fig.axis(ymin=0, ymax=800000)\n",
    "plt.show()\n",
    "\n",
    "df_train.drop(df_train[(df_train.OverallQual==4) & (df_train.SalePrice>200000)].index,inplace=True)\n",
    "\n",
    "df_train.plot.scatter(x='LotFrontage', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "df_train.drop(df_train[df_train['LotFrontage'] > 200].index,inplace=True)\n",
    "\n",
    "df_train.plot.scatter(x='LotArea', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "df_train.plot.scatter(x='YearBuilt', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "df_train.drop(df_train[(df_train.YearBuilt < 1900) & (df_train.SalePrice > 200000)].index,inplace=True)\n",
    "df_train.drop(df_train[(df_train.YearBuilt < 2000) & (df_train.SalePrice > 650000)].index,inplace=True)\n",
    "df_train.plot.scatter(x='YearBuilt', y='SalePrice')\n",
    "plt.show()\n",
    "\n",
    "corrmat1=df_train.corr()\n",
    "print(corrmat1['SalePrice'].sort_values(ascending=False).head(10))\n",
    "\n",
    "# ------------------------\n",
    "# Concatenate train & test\n",
    "# ------------------------\n",
    "target=df_train['SalePrice'].reset_index(drop=True)\n",
    "trainx=df_train.drop(['SalePrice'],axis=1)\n",
    "all_features=pd.concat([trainx,df_test]).reset_index(drop=True)\n",
    "\n",
    "# ------------------------\n",
    "# Missing data analysis\n",
    "# ------------------------\n",
    "total = all_features.isnull().sum().sort_values(ascending=False)\n",
    "percent = (all_features.isnull().sum()/all_features.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(missing_data.head(25))\n",
    "missing_data['Percent'].head(15).plot.bar()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------\n",
    "# Imputing missing & basic transformations\n",
    "# ------------------------\n",
    "all_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "all_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\n",
    "all_features['GarageArea'] = all_features.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\n",
    "all_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "all_features['YrSold'] = all_features['YrSold'].astype(str)\n",
    "all_features['MoSold'] = all_features['MoSold'].astype(str)\n",
    "all_features['Functional'] = all_features['Functional'].fillna('Typ')\n",
    "all_features['Electrical'] = all_features['Electrical'].fillna(\"SBrkr\")\n",
    "all_features['KitchenQual'] = all_features['KitchenQual'].fillna(\"TA\")\n",
    "all_features['Exterior1st'] = all_features['Exterior1st'].fillna(all_features['Exterior1st'].mode()[0])\n",
    "all_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(all_features['Exterior2nd'].mode()[0])\n",
    "all_features['SaleType'] = all_features['SaleType'].fillna(all_features['SaleType'].mode()[0])\n",
    "all_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "objects = []\n",
    "for i in all_features.columns:\n",
    "    if all_features[i].dtype == object:\n",
    "        objects.append(i)\n",
    "all_features.update(all_features[objects].fillna('None'))\n",
    "\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric = []\n",
    "for i in all_features.columns:\n",
    "    if all_features[i].dtype in numeric_dtypes:\n",
    "        numeric.append(i)\n",
    "all_features.update(all_features[numeric].fillna(0))\n",
    "\n",
    "# ------------------------\n",
    "# Feature engineering\n",
    "# ------------------------\n",
    "all_features['YearRemodAdd']=all_features['YearRemodAdd'].astype(int)\n",
    "all_features['Years_Since_Remod'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\n",
    "all_features['Age']=all_features['YrSold'].astype(int) - all_features['YearBuilt'].astype(int)\n",
    "all_features['Newness']=all_features['Age']*all_features['Years_Since_Remod']\n",
    "all_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n",
    "\n",
    "feats=['2ndFlrSF','GarageArea','TotalBsmtSF','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']\n",
    "for col  in feats:\n",
    "    name='Has_'+str(col)\n",
    "    all_features[name]=all_features[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "all_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] \n",
    "                                  +all_features['EnclosedPorch'] + all_features['ScreenPorch'] \n",
    "                                  +all_features['WoodDeckSF'])\n",
    "\n",
    "all_features['Bsmt_Baths'] = all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath'])\n",
    "all_features['Total_BathAbvGrd'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']))\n",
    "all_features['AvgRoomSize']=all_features['GrLivArea']/(all_features['TotRmsAbvGrd']+(0.4*all_features['Total_BathAbvGrd']))\n",
    "all_features['BedBath']=all_features['BedroomAbvGr']*all_features['Total_BathAbvGrd']\n",
    "all_features['TotalLot'] = all_features['LotFrontage'] + all_features['LotArea']\n",
    "all_features['sqft_feet_living']=all_features['TotalBsmtSF']+all_features['GrLivArea']\n",
    "\n",
    "all_features.drop(['Id','BsmtFullBath','BsmtHalfBath'],axis=1,inplace=True)\n",
    "\n",
    "# Neighborhood mapping\n",
    "neigh_map={'None': 0,'MeadowV':1,'IDOTRR':1,'BrDale':1,\n",
    "           'OldTown':2,'Edwards':2,'BrkSide':2,\n",
    "           'Sawyer':3,'Blueste':3,'SWISU':3,'NAmes':3,\n",
    "           'NPkVill':4,'Mitchel':4,'SawyerW':4,\n",
    "           'Gilbert':5,'NWAmes':5,'Blmngtn':5,\n",
    "           'CollgCr':6,'ClearCr':6,'Crawfor':6,\n",
    "           'Somerst':8,'Veenker':8,'Timber':8,\n",
    "           'StoneBr':10,'NoRidge':10,'NridgHt':10 } \n",
    "all_features['Neighborhood'] = all_features['Neighborhood'].map(neigh_map)\n",
    "\n",
    "# Quality maps\n",
    "bsm_map = {'None': 0, 'Po': 1, 'Fa': 4, 'TA': 9, 'Gd': 16, 'Ex': 25}\n",
    "ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond','FireplaceQu']\n",
    "for col in ord_col:\n",
    "    all_features[col] = all_features[col].map(bsm_map)\n",
    "\n",
    "# Fix year data\n",
    "all_features.loc[2284,'Years_Since_Remod']=0\n",
    "all_features.loc[2538,'Years_Since_Remod']=0\n",
    "all_features.loc[2538,'Age']=0\n",
    "\n",
    "# Drop highly collinear\n",
    "all_features.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','LotFrontage'], axis=1, inplace=True)\n",
    "# Drop features with huge missing\n",
    "all_features.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)\n",
    "# Drop less relevant\n",
    "all_features.drop(['MoSold','YrSold'], axis=1, inplace=True)\n",
    "\n",
    "# Drop features with >97% same values\n",
    "overfit_cat = []\n",
    "for i in all_features.columns:\n",
    "    counts = all_features[i].value_counts()\n",
    "    zeros = counts.iloc[0]\n",
    "    if zeros / len(all_features) * 100 > 97:\n",
    "        overfit_cat.append(i)\n",
    "overfit_cat=['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'LowQualFinSF', '3SsnPorch', 'Has_TotalBsmtSF','Has_3SsnPorch']\n",
    "all_features.drop(overfit_cat,axis=1,inplace=True)\n",
    "\n",
    "# ------------------------\n",
    "# Normalizing numeric features\n",
    "# ------------------------\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric = []\n",
    "for i in all_features.columns:\n",
    "    if all_features[i].dtype in numeric_dtypes:\n",
    "        numeric.append(i)\n",
    "\n",
    "skew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "for i in skew_index:\n",
    "    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))\n",
    "\n",
    "# ------------------------\n",
    "# Transform target\n",
    "# ------------------------\n",
    "target = np.log1p(df_train['SalePrice']).reset_index(drop=True)\n",
    "sns.distplot(target, fit=norm)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------\n",
    "# Additional feature transforms\n",
    "# (logging, squaring, sqrt, etc.)\n",
    "# ------------------------\n",
    "def logs(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)\n",
    "        res.columns.values[m] = l + '_log'\n",
    "        m += 1\n",
    "    return res\n",
    "\n",
    "log_features = ['BsmtUnfSF', 'TotalBsmtSF', 'GrLivArea', 'FireplaceQu', 'GarageArea', \n",
    "                'OpenPorchSF', 'EnclosedPorch','ScreenPorch','Years_Since_Remod',\n",
    "                'Newness','Total_Home_Quality','Total_porch_sf','AvgRoomSize',\n",
    "                'TotalLot','sqft_feet_living','BsmtFinSF1','BedBath']\n",
    "\n",
    "all_features1 = all_features.copy()\n",
    "loged_features = logs(all_features.copy(), numeric)\n",
    "all_features = logs(all_features, log_features)\n",
    "\n",
    "feats=['2ndFlrSF','GarageArea','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','ScreenPorch']\n",
    "for col  in feats:\n",
    "    name1='Has_'+str(col) + '_log'\n",
    "    if name1 in loged_features.columns:\n",
    "        loged_features.drop(name1,axis=1,inplace=True)\n",
    "\n",
    "for o in ord_col:\n",
    "    name=str(o)+'_log'\n",
    "    if name in loged_features.columns:\n",
    "        loged_features.drop(name,axis=1,inplace=True)\n",
    "\n",
    "if 'Neighborhood_log' in loged_features.columns:\n",
    "    loged_features.drop('Neighborhood_log',axis=1,inplace=True)\n",
    "\n",
    "def squares(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)\n",
    "        res.columns.values[m] = l + '_sq'\n",
    "        m += 1\n",
    "    return res \n",
    "\n",
    "sq_features = squares(all_features.copy(), list(all_features.columns))\n",
    "log_sq_cols = squares(loged_features.copy(), list(loged_features.columns))\n",
    "\n",
    "def sqrt(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(np.sqrt(res[l])).values)\n",
    "        res.columns.values[m] = l + '_sqroot'\n",
    "        m += 1\n",
    "    return res \n",
    "\n",
    "def cube(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(res[l]**3).values)\n",
    "        res.columns.values[m] = l + '_cube3'\n",
    "        m += 1\n",
    "    return res\n",
    "\n",
    "root_features = sqrt(sq_features.copy(), list(sq_features.columns))\n",
    "cube_features = cube(sq_features.copy(), list(sq_features.columns))\n",
    "var_feats = cube(log_sq_cols.copy(), list(log_sq_cols.columns))\n",
    "var_feats = sqrt(var_feats.copy(), list(log_sq_cols.columns))\n",
    "\n",
    "# ------------------------\n",
    "# One-hot encoding\n",
    "# ------------------------\n",
    "all_features = pd.get_dummies(all_features).reset_index(drop=True)\n",
    "all_features1 = pd.get_dummies(all_features1).reset_index(drop=True)\n",
    "loged_features = pd.get_dummies(loged_features).reset_index(drop=True)\n",
    "cube_features = pd.get_dummies(cube_features).reset_index(drop=True)\n",
    "var_feats = pd.get_dummies(var_feats).reset_index(drop=True)\n",
    "log_sq_cols = pd.get_dummies(log_sq_cols).reset_index(drop=True)\n",
    "\n",
    "def get_splits(all_features,target):\n",
    "    df=pd.concat([all_features, target],axis=1)\n",
    "    X_train=df.iloc[:len(target),:]\n",
    "    X_test=all_features.iloc[len(target):,:]\n",
    "    return X_train,X_test\n",
    "\n",
    "def get_valid(df,target,valid_fraction=0.2):\n",
    "    validrows=int(len(df)*valid_fraction)\n",
    "    trains=df[:-validrows]\n",
    "    valids=df[-validrows:]\n",
    "    feature_col=df.columns.drop(target)\n",
    "    return trains,valids,feature_col\n",
    "\n",
    "X_train,X_test=get_splits(all_features,target)\n",
    "train,valid,feature_col=get_valid(X_train,'SalePrice')\n",
    "\n",
    "X_train0,X_test0=get_splits(all_features1,target)\n",
    "train0,valid0,feature_col0=get_valid(X_train0,'SalePrice')\n",
    "\n",
    "X_train1,X_test1=get_splits(log_sq_cols,target)\n",
    "train1,valid1,feature_col1=get_valid(X_train1,'SalePrice')\n",
    "\n",
    "X_tr, X_te=get_splits(var_feats,target)\n",
    "feat_col=X_tr.columns.drop('SalePrice')\n",
    "\n",
    "tr_log,te_log=get_splits(loged_features,target)\n",
    "log_feats=tr_log.columns.drop('SalePrice')\n",
    "\n",
    "corr1=X_tr.corr()\n",
    "print(corr1[\"SalePrice\"].sort_values(ascending=False).head(15))\n",
    "\n",
    "best_columns=['sqft_feet_living','GarageArea_log_sq','Age','BedBath','AvgRoomSize_log_sq',\n",
    "              'TotalLot_log_sq','Total_porch_sf_log_sq']\n",
    "\n",
    "fig = plt.figure(figsize=(12,20))\n",
    "plt.subplots_adjust(right=2)\n",
    "plt.subplots_adjust(top=2)\n",
    "sns.color_palette(\"husl\", 8)\n",
    "\n",
    "for i, feature in enumerate(list(best_columns), 1):\n",
    "    plt.subplot(len(list(best_columns)), 2, i)\n",
    "    sns.scatterplot(x=feature, y='SalePrice', data=X_train)\n",
    "    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n",
    "    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n",
    "    plt.legend(loc='best', prop={'size': 10})\n",
    "plt.show()\n",
    "\n",
    "# ------------------------\n",
    "# Models & CV\n",
    "# ------------------------\n",
    "from sklearn.linear_model import Lasso\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "kf = KFold(n_splits=12, random_state=7, shuffle=True)\n",
    "def cv_rmse(model, X, target=target):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, target, scoring=\"neg_mean_squared_error\", cv=kf,n_jobs=-1))\n",
    "    return (rmse)\n",
    "\n",
    "# ------------------------\n",
    "# Lasso for feature selection\n",
    "# ------------------------\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "X = X_train1[feature_col1]\n",
    "y = target\n",
    "alphas = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,5e-3,5e-2]\n",
    "Xscaled=RobustScaler().fit_transform(X)\n",
    "lassopipe = LassoCV(max_iter=1e7, alphas=alphas, random_state=42,n_jobs=-1)\n",
    "clf=lassopipe.fit(Xscaled,y)\n",
    "importance=np.abs(clf.coef_)\n",
    "frame=pd.DataFrame(importance,index=feature_col1)\n",
    "imps=frame.sort_values(by=0,ascending=False).index\n",
    "\n",
    "# ------------------------\n",
    "# Ridge model\n",
    "# ------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "alphas=np.linspace(1,stop=50)\n",
    "pipe=Pipeline([('scaler',RobustScaler()),\n",
    "               ('ridgemodel',RidgeCV(alphas=alphas, cv=None,store_cv_values=True ))])\n",
    "model=pipe.fit(X_train1[feature_col1],target)\n",
    "print('Alpha: %f'%model['ridgemodel'].alpha_)\n",
    "print('Score: %f'%np.sqrt(-model['ridgemodel'].best_score_))\n",
    "\n",
    "ridgepred=model.predict(X_test1[feature_col1])\n",
    "\n",
    "# Trying different subset sizes\n",
    "alphas=np.linspace(1,stop=50,num=50)\n",
    "pipe=Pipeline([('scaler',RobustScaler()),\n",
    "               ('ridgemodel',RidgeCV(alphas=alphas, cv=None,store_cv_values=True))])\n",
    "numfeats=[298,290,280,275,270,260,250]\n",
    "for n in numfeats:\n",
    "    model=pipe.fit(X_train1[imps[:n]],target)\n",
    "    print(n)\n",
    "    print('Alpha: %f'%model['ridgemodel'].alpha_)\n",
    "    print('Score: %f'%np.sqrt(-model['ridgemodel'].best_score_))\n",
    "    print('')\n",
    "\n",
    "model=pipe.fit(X_train1[imps[:280]],target)\n",
    "ridgepred=model.predict(X_test1[imps[:280]])\n",
    "\n",
    "# ------------------------\n",
    "# LightGBM\n",
    "# ------------------------\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                         num_leaves=5,\n",
    "                         learning_rate=0.007,\n",
    "                         n_estimators=3500,\n",
    "                         max_bin=163,\n",
    "                         bagging_fraction=0.35711,\n",
    "                         bagging_freq=4,\n",
    "                         bagging_seed=8,\n",
    "                         feature_fraction=0.1294,\n",
    "                         feature_fraction_seed=8,\n",
    "                         min_data_in_leaf = 8,\n",
    "                         verbose=-1,\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "lightgbmod=lightgbm.fit(X_train[feature_col],target)\n",
    "lightpred=lightgbmod.predict(X_test[feature_col])\n",
    "print((np.sqrt(-cross_val_score(lightgbm, X_train[feature_col], target, \n",
    "                                scoring=\"neg_mean_squared_error\", cv=5))).mean())\n",
    "\n",
    "# ------------------------\n",
    "# SVR\n",
    "# ------------------------\n",
    "svr = make_pipeline(RobustScaler(),\n",
    "                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n",
    "svrmodel=svr.fit(X_train[feature_col],target)\n",
    "svrpred=svrmodel.predict(X_test[feature_col])\n",
    "print((cv_rmse(svr,X_train[feature_col])).mean())\n",
    "\n",
    "# ------------------------\n",
    "# XGBoost & Stacking\n",
    "# ------------------------\n",
    "xgboost = XGBRegressor(\n",
    "    learning_rate=0.0139,\n",
    "    n_estimators=4500,\n",
    "    max_depth=4,\n",
    "    min_child_weight=0,\n",
    "    subsample=0.7968,\n",
    "    colsample_bytree=0.4064,\n",
    "    nthread=-1,\n",
    "    scale_pos_weight=2,\n",
    "    seed=42,\n",
    ")\n",
    "xgboo=xgboost.fit(X_train[feature_col],target)\n",
    "xgboostpred=xgboo.predict(X_test[feature_col])\n",
    "\n",
    "stack_gen = StackingCVRegressor(regressors=(lightgbm, pipe, svr, xgboost),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)\n",
    "stack=stack_gen.fit(np.array(X_train[feature_col]),np.array(target))\n",
    "stackpreds=stack.predict(np.array(X_test[feature_col]))\n",
    "print((cv_rmse(xgboost,X_train[feature_col])).mean())\n",
    "\n",
    "# ------------------------\n",
    "# Blended model\n",
    "# ------------------------\n",
    "def submission(preds,pred):\n",
    "    submission=pd.read_csv(\"../input/home-data-for-ml-course/sample_submission.csv\")\n",
    "    submission.iloc[:,1] = np.floor(np.expm1(preds))\n",
    "\n",
    "    q1 = submission['SalePrice'].quantile(0.0045)\n",
    "    q2 = submission['SalePrice'].quantile(0.99)\n",
    "    submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n",
    "    submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n",
    "    # Scale predictions slightly\n",
    "    submission['SalePrice'] *= 1.001619\n",
    "    submission.to_csv('%s mycsvfile.csv'%pred,index=False)\n",
    "\n",
    "subm= (0.2 * lightpred)+(0.4 * ridgepred)+(0.1 * stackpreds)+(0.3*svrpred)\n",
    "submission(subm,pred='blended1705')\n",
    "\n",
    "# ------------------------\n",
    "# Permutation Importance demonstration\n",
    "# ------------------------\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "fc1=all_features1.drop(['sqft_feet_living','Age','Newness','Years_Since_Remod',\n",
    "                        'Total_Home_Quality','TotalLot','Total_porch_sf',\n",
    "                        'Bsmt_Baths','Total_BathAbvGrd','AvgRoomSize','BedBath'],axis=1)\n",
    "fc=fc1.columns\n",
    "\n",
    "alphas=np.linspace(1,stop=50,num=50)\n",
    "pipe0=Pipeline([('scaler',RobustScaler()), \n",
    "                ('ridgemodel',RidgeCV(alphas=alphas, cv=None,store_cv_values=True))])\n",
    "model0=pipe0.fit(X_train0[fc],target)\n",
    "\n",
    "perm0 = PermutationImportance(model0['ridgemodel'], random_state=1, cv=5).fit(X_train0[fc], target)\n",
    "# eli5.show_weights(perm0, feature_names=X_train0[fc].columns.tolist())\n",
    "\n",
    "pipe1=Pipeline([('scaler',RobustScaler()),\n",
    "                ('ridgemodel',RidgeCV(alphas=alphas, cv=None,store_cv_values=True))])\n",
    "x = X_train1[imps[:270]]\n",
    "model1=pipe1.fit(x,target)\n",
    "perm1 = PermutationImportance(model1['ridgemodel'], random_state=1,cv=5).fit(x, target)\n",
    "# eli5.show_weights(perm1, feature_names =x.columns.tolist())\n",
    "\n",
    "# ------------------------\n",
    "# Example shap usage\n",
    "# ------------------------\n",
    "import shap\n",
    "explainer = shap.LinearExplainer(model1['ridgemodel'], x)\n",
    "shap_values = explainer.shap_values(x)\n",
    "# shap.summary_plot(shap_values, x, max_display=6)\n",
    "\n",
    "# Example for XGBoost + SHAP\n",
    "# (requires supporting environment with shap & xgboost)\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xgb_full = xgboost.DMatrix(X_train[feature_col], label=target)\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_train[feature_col], target, \n",
    "                                                        test_size=0.2, random_state=7)\n",
    "xgb_train = xgboost.DMatrix(Xg_train, label=yg_train)\n",
    "xgb_test = xgboost.DMatrix(Xg_test, label=yg_test)\n",
    "params = {\n",
    "    'learning_rate':0.0139,\n",
    "    'n_estimators':4500,\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':0,\n",
    "    'subsample':0.7968,\n",
    "    'colsample_bytree':0.4064,\n",
    "    'nthread':-1,\n",
    "    'scale_pos_weight':2,\n",
    "    'seed':42,\n",
    "}\n",
    "\n",
    "modelxg = xgboost.train(params, xgb_train, 4500, \n",
    "                        [(xgb_train, \"train\"),(xgb_test, \"valid\")], \n",
    "                        early_stopping_rounds=5, verbose_eval=25)\n",
    "\n",
    "shap.initjs()\n",
    "shap_values1 = shap.TreeExplainer(modelxg).shap_values(xgb_test)\n",
    "# shap.summary_plot(shap_values1, xgb_test, max_display=12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
